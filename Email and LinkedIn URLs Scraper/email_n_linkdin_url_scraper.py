from dotenv import load_dotenv
import os
import requests
from typing import List, Dict, Any, Union
from autogen import UserProxyAgent, AssistantAgent, GroupChat, GroupChatManager
import sys

load_dotenv()

config_list=[
    {
        "model": os.getenv("AZURE_OPENAI_MODEL"),
        "api_key": os.getenv("AZURE_OPENAI_API_KEY"),
        "azure_endpoint": os.getenv("AZURE_OPENAI_ENDPOINT"),
        "api_type": "azure",
        "api_version": os.getenv("OPENAI_API_VERSION"),
    }
]

API_KEY = os.getenv("SERPER_API_KEY")

# Google search function with type hints
def search_google(query: str, num_results: int = 100) -> Union[List[Dict[str, Any]], None]:
    url = "https://google.serper.dev/search"
    headers = {
        "X-API-KEY": API_KEY,
        "Content-Type": "application/json"
    }
    data = {
        "q": query,
        "num": num_results
    }

    response = requests.post(url, headers=headers, json=data)

    if response.status_code == 200:
        search_results = response.json()
        if "organic" in search_results:
            return search_results["organic"]
        else:
            print(f"No search results for query: {query}")
            return []
    else:
        print(f"Failed to retrieve content for query: {query}, Status Code: {response.status_code}")
        return []
    
def store_keywords(keywords: List[str], file_path: str = "keywords.txt"):
    """
    Stores a list of keywords in a specified text file.

    Args:
        keywords (List[str]): A list of keywords to store in the file.
        file_path (str): Path to the file where keywords will be saved. Defaults to "keywords.txt".
    """
    try:
        with open(file_path, 'w') as file:
            for keyword in keywords:
                file.write(f"{keyword}\n")
        print(f"Keywords successfully saved to {file_path}")
    except Exception as e:
        print(f"An error occurred while saving keywords: {e}")


llm_config_search_google = {
    "functions": [
        {
            "name": "search_google",
            "description": "Perform a Google search to find target audience related to the product, service, or business described.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Search query for finding target audience related to LinkedIn lead generation.",
                    },
                    "num_results": {
                        "type": "integer",
                        "description": "The number of search results to retrieve, typically set to 100.",
                        "default": 100
                    }
                },
                "required": ["query"]
            }
        },
        {
            "name": "store_keywords",
            "description": "Store the keywords generated by other agents in a structured format for easy access and use in LinkedIn lead generation.",
            "parameters": {
                "type": "object",
                "properties": {
                    "keywords": {
                        "type": "array",
                        "description": "The list of keywords to store.",
                        "items": {
                            "type": "string"
                        }
                    }
                },
                "required": ["keywords"]
            }
        }
        
    ],
    "config_list": config_list  # Ensure to use your predefined config list here
}

# Define User Proxy Agent
user_proxy = UserProxyAgent(
    name="user_proxy",
    human_input_mode="NEVER",
    system_message="""You are a coordinator for specialized agents, tasked with assisting users in identifying their target audience based on the product, service, or business they describe. Collaborate with each agent to compile insights focusing solely on relevant keywords in the specified format for effective LinkedIn lead generation. Provide the output under the 'Target Audience' heading in a numbered list.""",
    code_execution_config={"last_n_messages": 2, "work_dir": "coding", "use_docker": False,},
    is_termination_msg=lambda x: x.get("content", "") and x.get("content", "").rstrip().endswith("TERMINATE"),
    function_map={"search_google": search_google, "store_keywords": store_keywords},
)

# Define Web Scraper Agent
web_scraper_agent = AssistantAgent(
    name="web_scraper_agent",
    llm_config={"config_list": config_list},
    system_message="""As the Web Scraper Agent, your task is to access the provided website URL and extract relevant keywords about the product, service, or business. Focus exclusively on keywords that highlight the target audience and related industry terms. Provide the output under the 'Target Audience' heading in a numbered list.""",
)

# Define Market Research Agent
market_research_agent = AssistantAgent(
    name="market_research_agent",
    llm_config={"config_list": config_list},
    system_message="""As the Market Research Agent, analyze current trends and demand in the industry related to the product, service, or business described. Identify only keywords relevant to the target audience and present them in a numbered list under the 'Target Audience' heading.""",
)

# Define Data-Driven Role Extractor Agent
data_driven_role_extractor = AssistantAgent(
    name="data_driven_role_extractor",
    llm_config=llm_config_search_google,
    function_map={"search_google": search_google},
    system_message="""As the Data-Driven Extractor, leverage available data sources and search APIs to identify significant audience keywords linked to the product, service, or business described. Highlight only key titles and positions relevant for engaging potential leads on LinkedIn, presenting them in a numbered list under the 'Target Audience' heading.""",
)

# Define Refinement and Review Agent
refinement_review_agent = AssistantAgent(
    name="refinement_review_agent",
    llm_config={"config_list": config_list},
    system_message="""As the Refinement and Review Agent, assess and enhance the information compiled by other agents regarding keywords for the target audience of the user's product, service, or business. Ensure that the insights are relevant, effective for maximizing LinkedIn outreach, consist solely of keywords without duplicates, and are presented in a numbered list under the 'Target Audience' heading.""",
)

keyword_storage_agent = AssistantAgent(
    name="keyword_storage_agent",
    llm_config=llm_config_search_google,
    function_map={"store_keywords": store_keywords},
    system_message="""As the Keyword Storage Agent, store the keywords generated by other agents in a structured format for easy access and use in LinkedIn lead generation.""",
)

# Group Chat Setup
group_chat = GroupChat(
    agents=[web_scraper_agent, market_research_agent, data_driven_role_extractor, refinement_review_agent, keyword_storage_agent],
    messages=[],
    max_round=10
)

group_chat_manager = GroupChatManager(
    name="group_chat_manager",
    groupchat=group_chat,
    llm_config={"config_list": config_list},
)

# Initiate Chat with Product/Service Description and Website URL
user_proxy.initiate_chat(
    group_chat_manager,
    summary_method="last_msg",
    message= "Find the target audience and Generate LinkedIn search keywords for the following product/service/business: " + sys.argv[2]
)

#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

import requests
import random
from bs4 import BeautifulSoup
import re
import time
import os
from concurrent.futures import ThreadPoolExecutor
import csv
from dotenv import load_dotenv
import json
from tavily import TavilyClient
import sys
import pathlib
# Load environment variables
 
# Define the path to the CSV file
storage_path = pathlib.Path(sys.argv[1])
csv_file_path = storage_path / "emails.csv"
verified_csv_file_path = storage_path / "verified_emails.csv"
 
if not storage_path.exists():
    storage_path.mkdir(parents=True)

# Load environment variables
load_dotenv()

# Define the path to the CSV file
storage_path = pathlib.Path(sys.argv[1])
csv_file_path = storage_path / "emails.csv"
json_file_path = storage_path / "verified_emails.json"

if not storage_path.exists():
    storage_path.mkdir(parents=True)

if not csv_file_path.exists():
    print("CSV file does not exist")
    exit()

scrapeops_api_key = os.getenv('SCRAPEOPS_API_KEY')
prospeo_api_key = os.getenv("PROSPEO_API_KEY")

scrapeops_url = 'https://proxy.scrapeops.io/v1/'
prospeo_url = "https://api.prospeo.io/email-verifier"

# Define headers for Prospeo API
required_headers = {
    'Content-Type': 'application/json',
    'X-KEY': prospeo_api_key
}

# Step 1: Google scraping functions
def search_google(query, num_results=100):
    site_url = f"https://www.google.com/search?q={query}&num={num_results}"

    response = requests.get(
        url=scrapeops_url,
        params={
            'api_key': '65997a45-1097-41db-84eb-6da6b3b5016b',
            'url': site_url, 
        },
    )
    print("---------------------------------------------------------------------------------------------------------")
    # print(response.content)
    if response.status_code == 200:
        return response.text
    else:
        return None
    
#-----------------------------------------------------------------------------------------------------------
    
# Google Search API keys
api_keys = [
    "c5329c99e4d3457072093429a0ce770374a376c6e20234059b369647e9aac151",  # SerpAPI
    "68KAWCiUvHF454Q6tXykLyj1",  # SearchAPI
    "92f3dc3a0816a03bd6a95290fe1a7779ffebf1a1"  # SerperAPI
]

# Names for better logging
api_names = [
    "SerpAPI",
    "SearchAPI",
    "SerperAPI"
]

def search_with_api_key(query, api_key, engine='google'):
    # if api_key == api_keys[0]:  # SerpAPI
    #     from serpapi import GoogleSearch
    #     params = {
    #         "api_key": api_key,
    #         "engine": engine,
    #         "q": query,
    #         "location": "Austin, Texas, United States",
    #         "google_domain": "google.com",
    #         "gl": "us",
    #         "hl": "en"
    #     }
    #     search = GoogleSearch(params)
    #     results = search.get_dict()
    #     if results and 'organic_results' in results:
    #         return results['organic_results'], 200
    #     else:
    #         print(f"No search results in SerpAPI for query: {query}")
    #         return [], 200  # Return an empty list with status code 200

    # elif api_key == api_keys[1]:  # SearchAPI
    #     url = "https://www.searchapi.io/api/v1/search"
    #     params = {"engine": engine, "q": query, "api_key": api_key}
    #     response = requests.get(url, params=params)
    #     if response.status_code == 200:
    #         search_results = response.json()
    #         if "results" in search_results:
    #             return search_results["results"], 200
    #         elif "organic_results" in search_results:
    #             return search_results["organic_results"], 200
    #         else:
    #             print(f"No valid results in SearchAPI response for query: {query}")
    #             return [], 200
    #     else:
    #         print(f"Failed to retrieve content for query: {query}, Status Code: {response.status_code}")
    #         return [], response.status_code

    if api_key == api_keys[2]:  # SerperAPI
        url = "https://google.serper.dev/search"
        headers = {
            "X-API-KEY": api_key,
            "Content-Type": "application/json"
        }
        data = {
            "q": query,
            "num": 100
        }
        response = requests.post(url, headers=headers, json=data)
        if response.status_code == 200:
            search_results = response.json()
            if "organic" in search_results:
                return search_results["organic"], response.status_code
            else:
                print(f"No search results in SerperAPI for query: {query}")
                return [], response.status_code
        else:
            print(f"Failed to retrieve content for query: {query}, Status Code: {response.status_code}")
            return [], response.status_code

    return [], 400  # Default case if no API matched

    

#---------------------------------------------------------------------------------------------------------------------

def find_result(query):
    # Try each API key in order
    for i, api_key in enumerate(api_keys):
        results, status_code = search_with_api_key(query, api_key)
        
        # Check if results are empty or the status code indicates failure, then move to the next API
        if status_code == 200 and results:
            print(f"Using {api_names[i]} for query: {query}")
            return results  # Return the results if successful and not empty
        else:
            print(f"{api_names[i]} with key {api_key} failed with status code: {status_code} or empty results.")

    # If all four APIs fail, fallback to ScrapeOps
    print("All Google search APIs failed. Now calling ScrapeOps...")
    return search_google(query)


# Function to extract emails from Serper search results
def extract_emails(results):
    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
    emails = set()
    for result in results:
        snippet = result.get('snippet', '')
        found_emails = re.findall(email_pattern, snippet)
        emails.update(found_emails)
    return emails



def get_emails_from_google(query, num_results=100):
    html_content = find_result(query)
    if html_content:
        return extract_emails(html_content)
    else:
        return []
    
# Step 2: Save emails to CSV
def save_emails_to_csv(emails, filename=csv_file_path):
    file_exists = os.path.isfile(filename)
    
    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:
        csvwriter = csv.writer(csvfile)
        
        # Write the header only if the file doesn't already exist
        if not file_exists:
            csvwriter.writerow(['Email'])
        
        for email in emails:
            csvwriter.writerow([email])
    
    print(f"Emails appended to {filename}")

# Main function to scrape emails until 10 new emails are collected
def scrape_emails(max_emails=10):
    print("Scraping emails...")
    collected_emails = set()  # Using a set to avoid duplicates
    
    while len(collected_emails) < max_emails:
        print(f"Collected emails so far: {len(collected_emails)}")
        
        # Generate a query
        query = generate_query()
        print(f"Generated query: {query}")
        
        # Use Google search to get emails
        emails = get_emails_from_google(query)
        print(f"Emails found in query: {emails}")
        
        # Add new emails to the collected set (this automatically avoids duplicates)
        collected_emails.update(emails)
        
        # Save the new emails to the CSV file
        if emails:
            print("Total collected emails: ", len(collected_emails))
            save_emails_to_csv(emails)
        
        # Sleep to avoid hitting rate limits (uncomment this line if needed)
        # time.sleep(5)
        
        # Break out of the loop once 10 emails are collected
        if len(collected_emails) >= max_emails:
            break
    
    return collected_emails

# Generate a query using the keyword, audience, location, and domain agents
def generate_query():
    selected_keyword = keyword_generator_agent()
    audience = target_audience_selector_agent()
    location = location_selector_agent()
    email_domain = email_domain_selector_agent()
    
    return f'site:linkedin.com/in "{selected_keyword}" "{location}" "{email_domain}" ("posted" OR "activity" OR "shared" OR "recent")'

# Load keywords from the file
def load_keywords_from_file(file_path="keywords.txt"):
    try:
        with open(file_path, 'r') as file:
            keywords = [line.strip() for line in file if line.strip()]
        return keywords
    except FileNotFoundError:
        print(f"File {file_path} not found.")
        return []

# Agents to generate different parts of the query
def keyword_generator_agent():
    all_keywords = load_keywords_from_file("keywords.txt")
    if not all_keywords:
        all_keywords = [
            "Stock", "Trader", "Investment", "Trading", "Equity", "Broker", 
            "Mutual Fund", "Option Trading", "Algorithmic Trading", "Portfolio Manager"
        ]
    return random.choice(all_keywords)

# Agents to generate different parts of the query
# def keyword_generator_agent():
#     all_keywords = [
#         "Stock", "Trader", "Investment", "Trading", "Equity", "Broker", 
#         "Mutual Fund", "Option Trading", "Algorithmic Trading", "Portfolio Manager"
#     ]
#     return random.choice(all_keywords)

def location_selector_agent():
    location_keywords = ["U.S.", "New York, United States"]
    return random.choice(location_keywords)

def target_audience_selector_agent():
    target_audiences = ["founders", "CEO", "project managers", "entrepreneurs"]
    return random.choice(target_audiences)

def email_domain_selector_agent():
    email_domains = ["gmail.com", "yahoo.com", "outlook.com"]
    return random.choice(email_domains)

# Tavily API for LinkedIn URL extraction
def extract_linkedin_urls_with_tavily(query):
    tavily_client = TavilyClient(api_key="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")
    response = tavily_client.search(query)
    
    # Assuming the response contains search results, return those
    return response.get('results', [])

def extract_details_from_tavily(tavily_response):
    """
    Extract LinkedIn URL, full name, and company from Tavily API response.
    """
    # Check if the response is not empty and is a list
    if isinstance(tavily_response, list) and tavily_response:
        # Loop through each result in the response
        for result in tavily_response:
            linkedin_url = result.get('url', 'NA')
            title = result.get('title', '')
            full_name = 'NA'
            company_name = 'NA'
            
            # Debugging information
            print(f"Processing result: {result}")

            # Strip " | LinkedIn" from the title if present
            title = title.replace(" | LinkedIn", "").strip()

            # Assume the title format is "Name - Position - Company" or "Name - Company"
            if ' - ' in title:
                name_and_position = title.split(' - ')
                full_name = name_and_position[0].strip()
                
                # Extract the company name based on the presence of "LinkedIn"
                if len(name_and_position) > 2:
                    company_name = ' - '.join(name_and_position[1:]).replace(" - LinkedIn", "").strip()
                elif len(name_and_position) == 2:
                    company_name = name_and_position[1].replace(" - LinkedIn", "").strip()

            # Debugging output
            print(f"Extracted Full Name: {full_name}, Company Name: {company_name}, LinkedIn URL: {linkedin_url}")

            return full_name, company_name, linkedin_url  # Return on first valid result

    return 'NA', 'NA', 'NA'  # Return default if no valid result

def extract_details_from_google(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    linkedin_url = 'NA'
    full_name = 'NA'
    company_name = 'NA'
    
    for a_tag in soup.find_all('a', href=True):
        if 'linkedin.com/in/' in a_tag['href']:
            linkedin_url = a_tag['href']
            break
    
    if linkedin_url != 'NA':
        result_title = soup.find('h3').get_text() if soup.find('h3') else ''
        name_and_company = result_title.split(' - ')
        full_name = name_and_company[0].strip() if len(name_and_company) > 0 else 'NA'
        company_name = name_and_company[1].strip() if len(name_and_company) > 1 else 'NA'

    return full_name, company_name, linkedin_url

# Adjust the get_details_by_email function to use the new extractor
collected_urls = 0

def get_details_by_email(email, num_results=1):
    global collected_urls  # Declare collected_urls as a global variable
    
    # Check if we've already collected 10 URLs
    if collected_urls >= 10:
        print("Limit of 10 LinkedIn URLs reached. Stopping further extraction.")
        return 'NA', 'NA', 'NA'  # Return NA if the limit is reached

    query = f'site:linkedin.com/in "{email}"'
    print(f"Searching for {query} in Tavily API...")
    tavily_results = extract_linkedin_urls_with_tavily(query)
    
    # First attempt to extract LinkedIn URLs using Tavily
    full_name, company, linkedin_url = extract_details_from_tavily(tavily_results)
    print("Full name, company, and LinkedIn URL from Tavily: " + str(full_name) + ", " + str(company) + ", " + str(linkedin_url))
    
    if linkedin_url == 'NA':
        print(f"Tavily API failed. Now using Google scraping for {email}...")
        google_results = search_google(query)  # Replace with your actual Google search function
        
        # Extract details from Google search results
        full_name, company, linkedin_url = extract_details_from_google(google_results)

    # Increment collected_urls only if a valid LinkedIn URL is found
    if linkedin_url != 'NA':
        collected_urls += 1
        print("Collected URLs: " + str(collected_urls))
        
    # Return the collected details
    return full_name, company, linkedin_url





# Step 4: Verify email using Prospeo API
def verify_email_prospeo(email):
    data = {'email': email}
    response = requests.post(prospeo_url, json=data, headers=required_headers)
    if response.status_code == 200:
        return response.json()
    else:
        print(f"Prospeo failed for {email}. Status code: {response.status_code}")
        return None

# Step 5: Save verified data to CSV and JSON, ensuring no duplicates
def save_to_csv_one_by_one(email, name, position, linkedin_url, email_status, filename=verified_csv_file_path):
    file_exists = os.path.isfile(filename)
    existing_emails = set()

    if file_exists:
        # Read existing emails to avoid duplicates
        with open(filename, 'r', encoding='utf-8') as csvfile:
            csvreader = csv.reader(csvfile)
            next(csvreader, None)  # Skip header
            for row in csvreader:
                existing_emails.add(row[0])

    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:
        csvwriter = csv.writer(csvfile)
        if not file_exists:
            csvwriter.writerow(['Email', 'Name', 'Position', 'LinkedIn URL', 'EMAIL STATUS'])
        if email not in existing_emails:  # Check for duplicates
            csvwriter.writerow([email, name, position, linkedin_url, email_status])
            print(f"Saved {email} to {filename} (excluding duplicates).")
        else:
            print(f"Skipped {email} as it is a duplicate.")

def main():
    # Step 1: Scrape up to 10 new emails
    collected_emails = scrape_emails(max_emails=10)
    print(f"Total collected emails: {collected_emails}")
    
    # Step 2: Load already processed emails from JSON
    output_json_file = 'email_verifier_results.json'
    if os.path.exists(output_json_file):
        with open(output_json_file, 'r') as json_file:
            existing_results = json.load(json_file)
            email_lookup = {item["email"]: item for item in existing_results}
    else:
        existing_results = []
        email_lookup = {}

    # Step 3: Use ThreadPoolExecutor for concurrent email verification and detail extraction
    new_results = []
    num_appended = 0  # Track how many new emails have been appended
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(get_details_by_email, email): email for email in collected_emails}  # Use a dict to map futures to emails
        for future in futures:
            email = futures[future]
            try:
                full_name, company, linkedin_url = future.result()
                print(f"Details for {email}: {full_name}, {company}, {linkedin_url}")

                if linkedin_url != 'NA':
                    prospeo_result = verify_email_prospeo(email)
                    if prospeo_result:
                        email_status = prospeo_result.get('response', {}).get('email_status', 'Unknown')
                        print(f"Verifying {email}, Status: {email_status}")
                        save_to_csv_one_by_one(email, full_name, company, linkedin_url, email_status)
                        num_appended += 1  # Increment the count of appended results
                        new_results.append({
                            "email": email,
                            "full_name": full_name,
                            "company": company,
                            "linkedin_url": linkedin_url,
                            "verified_with": "Prospeo",
                            "prospeo_result": prospeo_result
                        })
                    else:
                        print(f"Could not verify {email}, saving as 'Not Verified'")
                        save_to_csv_one_by_one(email, full_name, company, linkedin_url, 'Not Verified')
                        num_appended += 1
                        new_results.append({
                            "email": email,
                            "full_name": full_name,
                            "company": company,
                            "linkedin_url": linkedin_url,
                            "verified_with": "Prospeo",
                            "prospeo_result": "Not Verified"
                        })
                    
                    print(f"Saved {email} to CSV")
                else:
                    print(f"No valid LinkedIn URL found for {email}. Skipping.")

            except Exception as e:
                print(f"Error processing {email}: {e}")
            
            if num_appended >= 10:
                print("Reached 10 new verified entries. Stopping...")
                break  # Stop after 10 new entries

    # Step 4: Save new results to JSON
    all_results = existing_results + new_results
    with open(output_json_file, 'w') as json_file:
        json.dump(all_results, json_file, indent=4)

if __name__ == "__main__":
    main()


